/**
 * Exploit Tests - Demonstrates actual vulnerabilities found in the markdown loader
 * 
 * These tests show real bugs and security issues
 */

import { describe, it, expect, beforeEach, afterEach } from 'vitest';
import * as fs from 'fs/promises';
import * as path from 'path';
import { MarkdownLoader } from '../../src/utils/markdown-loader';

describe('Markdown Loader Exploit Tests', () => {
  let tempDir: string;
  let loader: MarkdownLoader;

  beforeEach(async () => {
    tempDir = path.join(__dirname, 'exploit-test-data');
    await fs.mkdir(tempDir, { recursive: true });
    loader = new MarkdownLoader(tempDir);
  });

  afterEach(async () => {
    try {
      await fs.rm(tempDir, { recursive: true, force: true });
    } catch (error) {
      // Ignore cleanup errors
    }
  });

  describe('ðŸš¨ CONFIRMED BUGS', () => {
    it('BUG #1: Cache does NOT check file modification time', async () => {
      const testFile = path.join(tempDir, 'cache-bug.md');
      
      // Create initial file
      await fs.writeFile(testFile, `---
title: "Original Version"
secret: "classified"
---
# Original sensitive content`);

      // Load and cache the file
      const doc1 = await loader.loadFile(testFile);
      console.log('âœ… First load:', doc1.title, '| Secret:', doc1.metadata.secret);
      
      // Wait a bit and modify the file
      await new Promise(resolve => setTimeout(resolve, 10));
      
      await fs.writeFile(testFile, `---
title: "Hacker Modified Version"  
secret: "HACKED"
malicious: "injected"
---
# HACKED CONTENT - This should be visible but cache prevents it`);

      // Load again - BUG: Returns cached version, ignoring file changes!
      const doc2 = await loader.loadFile(testFile);
      
      console.log('ðŸš¨ CACHE BUG CONFIRMED!');
      console.log('   Expected: "Hacker Modified Version"');
      console.log('   Got:', doc2.title);
      console.log('   File modification ignored due to cache bug!');
      
      // Demonstrate the vulnerability: file changes are ignored
      expect(doc2.title).toBe('Original Version');
      expect(doc2.metadata.secret).toBe('classified');
      expect(doc2.metadata.malicious).toBeUndefined();
    });

    it('BUG #2: Empty search query returns ALL documents (info leak)', async () => {
      const trainingDir = path.join(tempDir, 'training');
      await fs.mkdir(trainingDir, { recursive: true });
      
      // Create documents with sensitive information
      await fs.writeFile(path.join(trainingDir, 'secret1.md'), `---
title: "TOP SECRET Document"
classification: "SECRET"
tags: ["classified", "sensitive"]
---
# Nuclear launch codes: 12345`);

      await fs.writeFile(path.join(trainingDir, 'secret2.md'), `---
title: "Company Financials"
classification: "CONFIDENTIAL"
tags: ["financial", "sensitive"]  
---
# Revenue: $10M, Profit: $2M`);

      await fs.writeFile(path.join(trainingDir, 'public.md'), `---
title: "Public Document"
classification: "PUBLIC"
tags: ["public"]
---
# This is public information`);

      const docs = await loader.loadTrainingData();
      
      // Legitimate searches work as expected
      const publicSearch = loader.searchDocuments('public', docs);
      console.log('âœ… Public search returned:', publicSearch.length, 'documents');
      
      // BUG: Empty query exposes all documents (including sensitive ones!)
      const emptySearch = loader.searchDocuments('', docs);
      
      console.log('ðŸš¨ INFO LEAK BUG CONFIRMED!');
      console.log('   Empty search query returned:', emptySearch.length, 'documents');
      console.log('   Should return 0, but exposes all documents including:');
      
      emptySearch.forEach(doc => {
        if (doc.metadata.classification !== 'PUBLIC') {
          console.log(`   - LEAKED: "${doc.title}" (${doc.metadata.classification})`);
        }
      });
      
      // This bug allows information leakage
      expect(emptySearch.length).toBeGreaterThan(0);
      expect(emptySearch.some(doc => doc.metadata.classification === 'SECRET')).toBe(true);
    });

    it('BUG #3: No cache invalidation leads to stale data serving', async () => {
      const testFile = path.join(tempDir, 'stale-data.md');
      
      // Create file with user data
      await fs.writeFile(testFile, `---
title: "User Profile"
username: "alice"
role: "user"
permissions: ["read"]
---
# Alice's profile`);

      // Load initial data
      const doc1 = await loader.loadFile(testFile);
      console.log('âœ… Initial load - Role:', doc1.metadata.role);
      
      // Admin updates user permissions on disk
      await fs.writeFile(testFile, `---
title: "User Profile"
username: "alice"
role: "admin"  
permissions: ["read", "write", "delete"]
---
# Alice's updated profile - now admin!`);

      // BUG: Application continues serving old cached data
      const doc2 = await loader.loadFile(testFile);
      
      console.log('ðŸš¨ STALE DATA BUG CONFIRMED!');
      console.log('   Disk file shows role: "admin"');
      console.log('   Cache returns role:', doc2.metadata.role);
      console.log('   Security implications: Alice gets old permissions!');
      
      // This could lead to privilege escalation bugs
      expect(doc2.metadata.role).toBe('user'); // Old cached data
      expect(doc2.metadata.permissions).toEqual(['read']); // Old permissions
    });

    it('BUG #4: Memory exhaustion through recursive data structures', async () => {
      const testFile = path.join(tempDir, 'memory-bomb.md');
      
      // Create a document that will consume excessive memory
      const recursiveData = {
        level1: {
          level2: {
            level3: {
              level4: {
                level5: {
                  // This could continue or contain arrays with many elements
                  bigArray: Array(10000).fill('memory consuming string'.repeat(100))
                }
              }
            }
          }
        }
      };
      
      const content = `---
title: "Memory Bomb"
recursiveData: ${JSON.stringify(recursiveData)}
---
# This document consumes excessive memory`;

      await fs.writeFile(testFile, content);
      
      const startMemory = process.memoryUsage().heapUsed;
      const doc = await loader.loadFile(testFile);
      const endMemory = process.memoryUsage().heapUsed;
      
      const memoryIncrease = (endMemory - startMemory) / 1024 / 1024;
      
      console.log('ðŸš¨ MEMORY EXHAUSTION BUG:');
      console.log('   Memory increase:', memoryIncrease.toFixed(2), 'MB');
      console.log('   Document size caused significant memory consumption');
      
      // The bug is that there's no limit on metadata size
      expect(memoryIncrease).toBeGreaterThan(1); // Should consume > 1MB
      expect(doc.metadata.recursiveData).toBeDefined();
    });

    it('BUG #5: ID collision allows document overwriting', async () => {
      const trainingDir = path.join(tempDir, 'training');
      await fs.mkdir(trainingDir, { recursive: true });
      
      // Create files with paths that normalize to similar IDs
      const file1 = path.join(trainingDir, 'important.md');
      const file2 = path.join(trainingDir, 'important..md'); // Extra dot
      const file3 = path.join(trainingDir, 'important...md'); // More dots
      
      await fs.writeFile(file1, `---
title: "Important Document"
priority: "high"
---
# Critical system information`);

      await fs.writeFile(file2, `---
title: "Malicious Document"
priority: "malicious"
---
# Hacker content trying to overwrite important doc`);

      await fs.writeFile(file3, `---
title: "Another Malicious Document"
priority: "evil"
---
# More hacker content`);

      const docs = await loader.loadDirectory('training');
      
      // Check if IDs collide (this creates a vulnerability)
      const ids = docs.map(doc => doc.id);
      const uniqueIds = new Set(ids);
      
      console.log('ðŸ“ File paths created:');
      docs.forEach(doc => console.log('   -', doc.filePath, 'â†’ ID:', doc.id));
      
      console.log('ðŸš¨ ID COLLISION CHECK:');
      console.log('   Total documents:', docs.length);
      console.log('   Unique IDs:', uniqueIds.size);
      
      if (uniqueIds.size < docs.length) {
        console.log('   ID COLLISION DETECTED! Documents can overwrite each other!');
        
        // Find which IDs collided
        const idCounts = {};
        ids.forEach(id => {
          idCounts[id] = (idCounts[id] || 0) + 1;
        });
        
        Object.entries(idCounts).forEach(([id, count]) => {
          if (count > 1) {
            console.log(`   - ID "${id}" used by ${count} documents`);
          }
        });
      }
      
      // The vulnerability exists if IDs are not unique
      expect(uniqueIds.size).toBe(ids.length);
    });
  });

  describe('ðŸ” Security Analysis', () => {
    it('demonstrates combined attack: cache + search + memory', async () => {
      console.log('ðŸŽ¯ COMBINED ATTACK SIMULATION:');
      console.log('   1. Upload malicious document with large metadata');
      console.log('   2. Cache prevents updates/fixes');  
      console.log('   3. Empty search exposes all data');
      console.log('   4. Memory exhaustion via metadata size');
      
      const attackFile = path.join(tempDir, 'attack.md');
      
      // Step 1: Upload malicious document
      const largeMetadata = Array(1000).fill(null).reduce((acc, _, i) => {
        acc[`field${i}`] = 'x'.repeat(1000); // 1KB per field = 1MB total
        return acc;
      }, {});
      
      await fs.writeFile(attackFile, `---
title: "Innocent Looking Document"
normalField: "normal"
${Object.entries(largeMetadata).map(([k, v]) => `${k}: "${v}"`).join('\n')}
---
# Looks normal but contains large metadata`);

      // Step 2: Load and cache (now stuck in cache)
      const doc1 = await loader.loadFile(attackFile);
      
      // Step 3: Try to "fix" the file
      await fs.writeFile(attackFile, `---
title: "Fixed Document"
normalField: "fixed"
---
# Attempted fix with minimal metadata`);

      // Step 4: Cache bug prevents the fix
      const doc2 = await loader.loadFile(attackFile);
      
      console.log('   âœ… Attack successful:');
      console.log('   - Large metadata cached:', Object.keys(doc2.metadata).length, 'fields');
      console.log('   - Fix attempt blocked by cache');
      console.log('   - System stuck serving malicious cached version');
      
      expect(Object.keys(doc2.metadata).length).toBeGreaterThan(100);
      expect(doc2.title).toBe('Innocent Looking Document'); // Cached version
    });
  });
});