/**
 * OpenAI LLMPort Implementation
 *
 * Direct OpenAI integration for ΞKernel
 */

import { LLMPort, LLMSpec, LLMResponse, LLMDelta } from '../xi-kernel';

interface OpenAIMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface OpenAIChatRequest {
  model: string;
  messages: OpenAIMessage[];
  temperature?: number;
  max_tokens?: number;
}

interface OpenAIChatResponse {
  id: string;
  choices: Array<{
    message: { content: string; role: string };
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
  model: string;
}

interface OpenAIEmbedResponse {
  data: Array<{ embedding: number[] }>;
}

export class OpenAIPort implements LLMPort {
  private apiKey: string;
  private model: string;
  private temperature: number;
  private maxTokens: number;
  private embedModel: string;

  constructor(config: {
    apiKey: string;
    model?: string;
    temperature?: number;
    maxTokens?: number;
    embedModel?: string;
  }) {
    this.apiKey = config.apiKey;
    this.model = config.model || 'gpt-4o-mini';
    this.temperature = config.temperature || 0.7;
    this.maxTokens = config.maxTokens || 2000;
    this.embedModel = config.embedModel || 'text-embedding-3-small';
  }

  async prompt(symbolId: string, spec: LLMSpec): Promise<LLMResponse> {
    const systemPrompt = `You are an AI operating in ΞKernel.\nSymbol: ${symbolId}\nTask: ${spec.task}\nContext: ${JSON.stringify(spec.context)}\nRespond clearly.`;

    const messages: OpenAIMessage[] = [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: spec.task }
    ];

    const request: OpenAIChatRequest = {
      model: this.model,
      messages,
      temperature: spec.constraints.temperature || this.temperature,
      max_tokens: spec.constraints.maxTokens || this.maxTokens
    };

    try {
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(request)
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`OpenAI API error (${response.status}): ${errorText}`);
      }

      const data: OpenAIChatResponse = await response.json();
      const content = data.choices[0]?.message?.content || '';

      return {
        payload: content,
        justification: `Generated by ${data.model}`,
        confidence: 0.8,
        tokensUsed: data.usage.total_tokens,
        model: data.model,
        cost: this.calculateCost(data.usage),
        timestamp: new Date(),
        metadata: { requestId: data.id, symbolId, provider: 'openai' }
      };
    } catch (error: any) {
      throw new Error(`OpenAI API failed: ${error.message}`);
    }
  }

  async critique(symbolId: string, target: Record<string, any>): Promise<LLMDelta[]> {
    const response = await this.prompt(`${symbolId}_critique`, {
      symbolId: `${symbolId}_critique`,
      task: `Analyze this and suggest one specific improvement: ${JSON.stringify(target)}`,
      context: { critiquing: true },
      constraints: { maxTokens: 200 }
    });

    return [{
      operation: 'update',
      target: symbolId,
      changes: { improvement: response.payload.toString().slice(0, 100) },
      reason: 'AI improvement suggestion',
      confidence: 0.7
    }];
  }

  async link(symbolA: string, symbolB: string, relationSpec: string): Promise<{relation: string, confidence: number}[]> {
    return [{ relation: relationSpec, confidence: 0.8 }];
  }

  async embed(payload: any): Promise<number[]> {
    const text = typeof payload === 'string' ? payload : JSON.stringify(payload);
    try {
      const response = await fetch('https://api.openai.com/v1/embeddings', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({ model: this.embedModel, input: text })
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`OpenAI API error (${response.status}): ${errorText}`);
      }

      const data: OpenAIEmbedResponse = await response.json();
      return data.data[0]?.embedding || [];
    } catch {
      const hash = this.hash(text);
      return Array(384).fill(0).map((_, i) => Math.sin(hash + i) * 0.5);
    }
  }

  private calculateCost(usage: { total_tokens: number }): number {
    return (usage.total_tokens / 1000) * 0.002;
  }

  private hash(str: string): number {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      hash = ((hash << 5) - hash + str.charCodeAt(i)) & 0xffffffff;
    }
    return Math.abs(hash);
  }
}

