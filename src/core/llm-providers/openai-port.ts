/**
 * OpenAI LLMPort Adapter
 */

import { LLMPort, LLMSpec, LLMResponse, LLMDelta } from '../xi-kernel';

interface OpenAIMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface OpenAIRequest {
  model: string;
  messages: OpenAIMessage[];
  temperature?: number;
  max_tokens?: number;
  seed?: number;
}

interface OpenAIResponse {
  id: string;
  choices: Array<{
    message: { content: string; role: string };
    finish_reason: string;
  }>;
  usage: { prompt_tokens: number; completion_tokens: number; total_tokens: number };
  model: string;
}

interface OpenAIEmbeddingRequest {
  model: string;
  input: string;
}

interface OpenAIEmbeddingResponse {
  data: Array<{ embedding: number[] }>;
}

export class OpenAIPort implements LLMPort {
  private apiKey: string;
  private model: string;
  private temperature: number;
  private maxTokens: number;
  private baseUrl = 'https://api.openai.com/v1';

  constructor(config: { apiKey: string; model?: string; temperature?: number; maxTokens?: number }) {
    this.apiKey = config.apiKey;
    this.model = config.model || 'gpt-4o-mini';
    this.temperature = config.temperature || 0.7;
    this.maxTokens = config.maxTokens || 2000;
  }

  async prompt(symbolId: string, spec: LLMSpec): Promise<LLMResponse> {
    const systemPrompt = this.buildSystemPrompt(symbolId, spec);
    const messages: OpenAIMessage[] = [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: spec.task }
    ];

    const request: OpenAIRequest = {
      model: this.model,
      messages,
      temperature: spec.constraints.temperature || this.temperature,
      max_tokens: spec.constraints.maxTokens || this.maxTokens,
      seed: Math.floor(Math.random() * 10000)
    };

    const response = await this.makeChatCall(request);
    const choice = response.choices[0];

    return {
      payload: choice?.message?.content || '',
      justification: `Generated by ${response.model} via OpenAI`,
      confidence: this.estimateConfidence(choice?.finish_reason),
      tokensUsed: response.usage.total_tokens,
      model: response.model,
      timestamp: new Date(),
      metadata: { requestId: response.id, symbolId, provider: 'openai' }
    };
  }

  async critique(symbolId: string, target: Record<string, any>): Promise<LLMDelta[]> {
    const spec: LLMSpec = {
      symbolId: `${symbolId}_critique`,
      task: `Review and suggest improvements for: ${JSON.stringify(target)}\nReturn lines as IMPROVE: field: suggestion: confidence`,
      context: { critiquing: symbolId },
      constraints: { maxTokens: 800, temperature: 0.6 }
    };

    const response = await this.prompt(spec.symbolId, spec);
    const deltas: LLMDelta[] = [];
    const matches = response.payload.toString().match(/IMPROVE:\s*([^:]+):\s*([^:]+):\s*([\d.]+)/gi);
    if (matches) {
      matches.forEach(m => {
        const parts = m.match(/IMPROVE:\s*([^:]+):\s*([^:]+):\s*([\d.]+)/i);
        if (parts) {
          deltas.push({
            operation: 'update',
            target: symbolId,
            changes: { [parts[1].trim()]: parts[2].trim() },
            reason: `AI critique suggestion: ${parts[2].trim()}`,
            confidence: parseFloat(parts[3])
          });
        }
      });
    }
    if (deltas.length === 0) {
      deltas.push({
        operation: 'update',
        target: symbolId,
        changes: { improvement: response.payload.toString().slice(0, 200) },
        reason: 'General AI improvement suggestion',
        confidence: response.confidence || 0.7
      });
    }
    return deltas;
  }

  async link(symbolA: string, symbolB: string, relationSpec: string): Promise<{relation: string, confidence: number}[]> {
    const spec: LLMSpec = {
      symbolId: `link_${symbolA}_${symbolB}`,
      task: `Analyze relationship between "${symbolA}" and "${symbolB}". Use relation type "${relationSpec}". Format lines as RELATION: name: confidence`,
      context: { linking: true },
      constraints: { maxTokens: 400, temperature: 0.5 }
    };
    const response = await this.prompt(spec.symbolId, spec);
    const relations: {relation: string, confidence: number}[] = [];
    const matches = response.payload.toString().match(/RELATION:\s*([^:]+):\s*([\d.]+)/gi);
    if (matches) {
      matches.forEach(m => {
        const parts = m.match(/RELATION:\s*([^:]+):\s*([\d.]+)/i);
        if (parts) {
          relations.push({ relation: parts[1].trim(), confidence: parseFloat(parts[2]) });
        }
      });
    }
    if (relations.length === 0) {
      relations.push({ relation: relationSpec, confidence: 0.7 });
    }
    return relations;
  }

  async embed(payload: any): Promise<number[]> {
    const text = typeof payload === 'string' ? payload : JSON.stringify(payload);
    const request: OpenAIEmbeddingRequest = { model: 'text-embedding-3-small', input: text };
    const response = await this.makeEmbeddingCall(request);
    return response.data[0]?.embedding || [];
  }

  private buildSystemPrompt(symbolId: string, spec: LLMSpec): string {
    return `You are an AI agent within ÎžKernel.
Symbol: ${symbolId}
Task: ${spec.task}
Context: ${JSON.stringify(spec.context)}
Respond precisely.`;
  }

  private async makeChatCall(request: OpenAIRequest): Promise<OpenAIResponse> {
    const res = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(request)
    });
    if (!res.ok) {
      const err = await res.text();
      throw new Error(`OpenAI API error (${res.status}): ${err}`);
    }
    return await res.json();
  }

  private async makeEmbeddingCall(request: OpenAIEmbeddingRequest): Promise<OpenAIEmbeddingResponse> {
    const res = await fetch(`${this.baseUrl}/embeddings`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(request)
    });
    if (!res.ok) {
      const err = await res.text();
      throw new Error(`OpenAI embedding error (${res.status}): ${err}`);
    }
    return await res.json();
  }

  private estimateConfidence(finishReason: string | undefined): number {
    switch (finishReason) {
      case 'stop': return 0.9;
      case 'length': return 0.7;
      case 'content_filter': return 0.3;
      default: return 0.6;
    }
  }
}

