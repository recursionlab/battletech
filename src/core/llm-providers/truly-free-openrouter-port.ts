/**
 * Truly Free OpenRouter Port - Correct $0.00 cost for Sonoma Dusk Alpha
 */

import { LLMPort, LLMSpec, LLMResponse, LLMDelta } from '../xi-kernel';

interface OpenRouterMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface OpenRouterRequest {
  model: string;
  messages: OpenRouterMessage[];
  temperature?: number;
  max_tokens?: number;
}

interface OpenRouterResponse {
  id: string;
  choices: Array<{
    message: {
      content: string;
      role: string;
    };
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
  model: string;
}

export class TrulyFreeOpenRouterPort implements LLMPort {
  private apiKey: string;
  private model: string;
  private temperature: number;
  private maxTokens: number;

  constructor(config: {
    apiKey: string;
    model?: string;
    temperature?: number;
    maxTokens?: number;
  }) {
    this.apiKey = config.apiKey;
    this.model = config.model || 'openrouter/sonoma-dusk-alpha';
    this.temperature = config.temperature || 0.7;
    this.maxTokens = config.maxTokens || 2000;
  }

  async prompt(symbolId: string, spec: LLMSpec): Promise<LLMResponse> {
    const systemPrompt = `You are an AI operating in XiKernel, a symbolic reasoning system.

Symbol: ${symbolId}
Task: ${spec.task}
Context: ${JSON.stringify(spec.context)}

Respond clearly and thoughtfully. Your output becomes part of a persistent knowledge graph.`;

    const messages: OpenRouterMessage[] = [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: spec.task }
    ];

    const request: OpenRouterRequest = {
      model: this.model,
      messages,
      temperature: this.temperature,
      max_tokens: this.maxTokens
    };

    try {
      const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(request)
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`OpenRouter API error (${response.status}): ${errorText}`);
      }

      const data: OpenRouterResponse = await response.json();
      const content = data.choices[0]?.message?.content || '';

      return {
        payload: content,
        justification: `Generated by ${data.model}`,
        confidence: 0.85,
        tokensUsed: data.usage.total_tokens,
        model: data.model,
        cost: this.calculateTrueCost(data.usage, data.model), // FIXED COST CALCULATION
        timestamp: new Date(),
        meta: {},
        metadata: {
          requestId: data.id,
          symbolId,
          provider: 'openrouter',
          actualModel: data.model,
          inputTokens: data.usage.prompt_tokens,
          outputTokens: data.usage.completion_tokens
        }
      };

    } catch (error) {
      throw new Error(`OpenRouter API failed: ${error.message}`);
    }
  }

  async critique(symbolId: string, target: Record<string, any>): Promise<LLMDelta[]> {
    const response = await this.prompt(`${symbolId}_critique`, {
      symbolId: `${symbolId}_critique`,
      task: `Analyze this content and suggest ONE specific improvement: ${JSON.stringify(target)}

Format your response as: "IMPROVE: [area]: [specific suggestion]: [confidence 0.0-1.0]"`,
      context: { critiquing: true },
      constraints: { maxTokens: 200 }
    });

    // Parse the response for structured improvements
    const improveMatch = response.payload.toString().match(/IMPROVE:\s*([^:]+):\s*([^:]+):\s*([\d.]+)/i);
    
    if (improveMatch) {
      return [{
        operation: 'update',
        target: symbolId,
        changes: { [improveMatch[1].trim()]: improveMatch[2].trim() },
        reason: `AI critique: ${improveMatch[2].trim()}`,
        confidence: parseFloat(improveMatch[3])
      }];
    }

    // Fallback if no structured response
    return [{
      operation: 'update',
      target: symbolId,
      changes: { improvement: response.payload.toString().slice(0, 100) },
      reason: 'AI general improvement suggestion',
      confidence: 0.7
    }];
  }

  async link(symbolA: string, symbolB: string, relationSpec: string): Promise<{relation: string, confidence: number}[]> {
    const response = await this.prompt(`link_${symbolA}_${symbolB}`, {
      symbolId: `link_analysis`,
      task: `Analyze the relationship between "${symbolA}" and "${symbolB}".
      
The suggested relationship type is: "${relationSpec}"

Respond with: "RELATION: [relationship_name]: [confidence 0.0-1.0]: [brief justification]"`,
      context: { linking: true, symbolA, symbolB },
      constraints: { maxTokens: 150 }
    });

    // Parse structured response
    const relationMatch = response.payload.toString().match(/RELATION:\s*([^:]+):\s*([\d.]+):\s*([^\n]+)/i);
    
    if (relationMatch) {
      return [{
        relation: relationMatch[1].trim(),
        confidence: parseFloat(relationMatch[2])
      }];
    }

    // Fallback
    return [{
      relation: relationSpec,
      confidence: 0.8
    }];
  }

  async embed(payload: any): Promise<number[]> {
    // Deterministic embedding based on content
    const text = JSON.stringify(payload);
    const hash = this.hash(text);
    return Array(384).fill(0).map((_, i) => Math.sin(hash + i * 0.1) * 0.5);
  }

  private calculateTrueCost(usage: any, model: string): number {
    // FREE MODELS - Return exactly $0.00
    const freeModels = [
      'openrouter/sonoma-dusk-alpha',
      'openrouter/auto', // Sometimes resolves to free models
    ];
    
    if (freeModels.some(freeModel => model.includes(freeModel) || model.includes('sonoma-dusk'))) {
      return 0.0; // TRULY FREE!
    }

    // Paid models cost calculation
    const costs: Record<string, { input: number; output: number }> = {
      'openai/gpt-3.5-turbo': { input: 0.0015 / 1000, output: 0.002 / 1000 },
      'openai/gpt-4o-mini': { input: 0.15 / 1000000, output: 0.6 / 1000000 },
      'anthropic/claude-3-haiku': { input: 0.25 / 1000000, output: 1.25 / 1000000 }
    };

    const modelCost = costs[model] || { input: 0.001 / 1000, output: 0.002 / 1000 };
    return (usage.prompt_tokens * modelCost.input) + (usage.completion_tokens * modelCost.output);
  }

  private hash(str: string): number {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      hash = ((hash << 5) - hash + str.charCodeAt(i)) & 0xffffffff;
    }
    return Math.abs(hash);
  }
}