/**
 * Anthropic LLMPort Adapter
 */

import { LLMPort, LLMSpec, LLMResponse, LLMDelta } from '../xi-kernel';

interface AnthropicRequest {
  model: string;
  max_tokens: number;
  messages: Array<{ role: 'user' | 'assistant'; content: string }>;
  system?: string;
  temperature?: number;
}

interface AnthropicResponse {
  id: string;
  content: Array<{ type: 'text'; text: string }>;
  model: string;
  usage: { input_tokens: number; output_tokens: number };
  stop_reason: string;
}

interface AnthropicEmbeddingRequest {
  model: string;
  input: string;
}

interface AnthropicEmbeddingResponse {
  embeddings: Array<{ embedding: number[] }>;
}

export class AnthropicPort implements LLMPort {
  private apiKey: string;
  private model: string;
  private temperature: number;
  private maxTokens: number;
  private baseUrl = 'https://api.anthropic.com/v1';

  constructor(config: { apiKey: string; model?: string; temperature?: number; maxTokens?: number }) {
    this.apiKey = config.apiKey;
    this.model = config.model || 'claude-3-sonnet-20240229';
    this.temperature = config.temperature || 0.7;
    this.maxTokens = config.maxTokens || 2000;
  }

  async prompt(symbolId: string, spec: LLMSpec): Promise<LLMResponse> {
    const system = this.buildSystemPrompt(symbolId, spec);
    const request: AnthropicRequest = {
      model: this.model,
      max_tokens: spec.constraints.maxTokens || this.maxTokens,
      temperature: spec.constraints.temperature || this.temperature,
      system,
      messages: [{ role: 'user', content: spec.task }]
    };

    const response = await this.makeChatCall(request);

    // Handle missing or malformed content and usage fields
    let text = '';
    if (
      response.content &&
      Array.isArray(response.content) &&
      response.content.length > 0 &&
      typeof response.content[0].text === 'string'
    ) {
      text = response.content[0].text;
    }

    let tokensUsed = 0;
    if (
      response.usage &&
      typeof response.usage.input_tokens === 'number' &&
      typeof response.usage.output_tokens === 'number'
    ) {
      tokensUsed = response.usage.input_tokens + response.usage.output_tokens;
    }

    return {
      payload: text,
      justification: `Generated by ${response.model ?? 'unknown model'} via Anthropic`,
      confidence: this.estimateConfidence(response.stop_reason),
      tokensUsed,
      model: response.model ?? 'unknown',
      timestamp: new Date(),
      metadata: { requestId: response.id ?? '', symbolId, provider: 'anthropic' }
    };
  }

  async critique(symbolId: string, target: Record<string, any>): Promise<LLMDelta[]> {
    const spec: LLMSpec = {
      symbolId: `${symbolId}_critique`,
      task: `Review and suggest improvements for: ${JSON.stringify(target)}\nReturn lines as IMPROVE: field: suggestion: confidence`,
      context: { critiquing: symbolId },
      constraints: { maxTokens: 800, temperature: 0.6 }
    };
    const response = await this.prompt(spec.symbolId, spec);
    const deltas: LLMDelta[] = [];
    const matches = response.payload.toString().match(/IMPROVE:\s*([^:]+):\s*([^:]+):\s*([\d.]+)/gi);
    if (matches) {
      matches.forEach(m => {
        const parts = m.match(/IMPROVE:\s*([^:]+):\s*([^:]+):\s*([\d.]+)/i);
        if (parts) {
          deltas.push({
            operation: 'update',
            target: symbolId,
            changes: { [parts[1].trim()]: parts[2].trim() },
            reason: `AI critique suggestion: ${parts[2].trim()}`,
            confidence: parseFloat(parts[3])
          });
        }
      });
    }
    if (deltas.length === 0) {
      deltas.push({
        operation: 'update',
        target: symbolId,
        changes: { improvement: response.payload.toString().slice(0, 200) },
        reason: 'General AI improvement suggestion',
        confidence: response.confidence || 0.7
      });
    }
    return deltas;
  }

  async link(symbolA: string, symbolB: string, relationSpec: string): Promise<{relation: string, confidence: number}[]> {
    const spec: LLMSpec = {
      symbolId: `link_${symbolA}_${symbolB}`,
      task: `Analyze relationship between "${symbolA}" and "${symbolB}". Use relation type "${relationSpec}". Format lines as RELATION: name: confidence`,
      context: { linking: true },
      constraints: { maxTokens: 400, temperature: 0.5 }
    };
    const response = await this.prompt(spec.symbolId, spec);
    const relations: {relation: string, confidence: number}[] = [];
    const matches = response.payload.toString().match(/RELATION:\s*([^:]+):\s*([\d.]+)/gi);
    if (matches) {
      matches.forEach(m => {
        const parts = m.match(/RELATION:\s*([^:]+):\s*([\d.]+)/i);
        if (parts) {
          relations.push({ relation: parts[1].trim(), confidence: parseFloat(parts[2]) });
        }
      });
    }
    if (relations.length === 0) {
      relations.push({ relation: relationSpec, confidence: 0.7 });
    }
    return relations;
  }

  async embed(payload: any): Promise<number[]> {
    const text = typeof payload === 'string' ? payload : JSON.stringify(payload);
    const request: AnthropicEmbeddingRequest = { model: 'claude-3-sonnet-20240229', input: text };
    const response = await this.makeEmbeddingCall(request);
    return response.embeddings[0]?.embedding || [];
  }

  private buildSystemPrompt(symbolId: string, spec: LLMSpec): string {
    return `You are Claude operating within ÎžKernel.
Symbol: ${symbolId}
Task: ${spec.task}
Context: ${JSON.stringify(spec.context)}
Respond precisely.`;
  }

  private async makeChatCall(request: AnthropicRequest): Promise<AnthropicResponse> {
    const res = await fetch(`${this.baseUrl}/messages`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
        'anthropic-version': '2023-06-01'
      },
      body: JSON.stringify(request)
    });
    if (!res.ok) {
      const err = await res.text();
      throw new Error(`Anthropic API error (${res.status}): ${err}`);
    }
    return await res.json();
  }

  private async makeEmbeddingCall(request: AnthropicEmbeddingRequest): Promise<AnthropicEmbeddingResponse> {
    const res = await fetch(`${this.baseUrl}/embeddings`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
        'anthropic-version': '2023-06-01'
      },
      body: JSON.stringify(request)
    });
    if (!res.ok) {
      const err = await res.text();
      throw new Error(`Anthropic embedding error (${res.status}): ${err}`);
    }
    return await res.json();
  }

  private estimateConfidence(stopReason: string | undefined): number {
    switch (stopReason) {
      case 'end_turn': return 0.9;
      case 'max_tokens': return 0.7;
      case 'stop_sequence': return 0.8;
      default: return 0.6;
    }
  }
}

