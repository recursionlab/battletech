/**
 * OpenRouter LLMPort Implementation
 * 
 * Implements the LLMPort interface for ΞKernel integration
 */

import { LLMPort, LLMSpec, LLMResponse, LLMDelta } from '../xi-kernel';

interface OpenRouterMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface OpenRouterRequest {
  model: string;
  messages: OpenRouterMessage[];
  temperature?: number;
  max_tokens?: number;
}

interface OpenRouterResponse {
  id: string;
  choices: Array<{
    message: {
      content: string;
      role: string;
    };
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
  model: string;
}

export class OpenRouterPort implements LLMPort {
  private apiKey: string;
  private model: string;
  private temperature: number;
  private maxTokens: number;
  private baseUrl: string = 'https://openrouter.ai/api/v1';

  constructor(config: {
    apiKey: string;
    model?: string;
    temperature?: number;
    maxTokens?: number;
    siteName?: string;
  }) {
    this.apiKey = config.apiKey;
    this.model = config.model || 'openrouter/sonoma-dusk-alpha';
    this.temperature = config.temperature || 0.7;
    this.maxTokens = config.maxTokens || 2000;
  }

  async prompt(symbolId: string, spec: LLMSpec): Promise<LLMResponse> {
    // Build context-aware system prompt
    const systemPrompt = this.buildSystemPrompt(symbolId, spec);

    const messages: OpenRouterMessage[] = [
      {
        role: 'system',
        content: systemPrompt
      },
      {
        role: 'user',
        content: spec.task
      }
    ];

    const request: OpenRouterRequest = {
      model: this.model,
      messages,
      temperature: spec.constraints.temperature || this.temperature,
      max_tokens: spec.constraints.maxTokens || this.maxTokens
    };

    try {
      const response = await this.makeAPICall(request);
      const content = response.choices[0]?.message?.content || '';

      return {
        payload: content,
        justification: `Generated by ${response.model} via OpenRouter`,
        confidence: this.estimateConfidence(response.choices[0]?.finish_reason),
        tokensUsed: response.usage.total_tokens,
        model: response.model,
        cost: this.calculateCost(response.usage, response.model),
        timestamp: new Date()
      };

    } catch (error) {
      throw new Error(`OpenRouter API call failed: ${error}`);
    }
  }

  async critique(symbolId: string, target: Record<string, any>): Promise<LLMDelta[]> {
    const spec: LLMSpec = {
      symbolId: `${symbolId}_critique`,
      task: `Analyze and suggest improvements for this content: ${JSON.stringify(target)}. 
      
      Provide specific, actionable suggestions with confidence ratings.
      Format as: IMPROVE: [area]: [specific suggestion]: [confidence 0-1]`,
      context: { critiquing: symbolId, target },
      constraints: { maxTokens: 800, temperature: 0.6 }
    };

    const response = await this.prompt(`${symbolId}_critique`, spec);
    
    // Parse response for deltas (simplified)
    const deltas: LLMDelta[] = [];
    const improveMatches = response.payload.toString().match(/IMPROVE:\s*([^:]+):\s*([^:]+):\s*([\d.]+)/gi);
    
    if (improveMatches) {
  improveMatches.forEach((match: string) => {
        const parts = match.match(/IMPROVE:\s*([^:]+):\s*([^:]+):\s*([\d.]+)/i);
        if (parts) {
          deltas.push({
            operation: 'update',
            target: symbolId,
            changes: { [parts[1].trim()]: parts[2].trim() },
            reason: `AI critique suggestion: ${parts[2].trim()}`,
            confidence: parseFloat(parts[3])
          });
        }
      });
    }

    // If no structured suggestions, create a general one
    if (deltas.length === 0) {
      deltas.push({
        operation: 'update',
        target: symbolId,
        changes: { improvement: response.payload.toString().slice(0, 200) },
        reason: 'General AI improvement suggestion',
        confidence: response.confidence || 0.7
      });
    }

    return deltas;
  }

  async link(symbolA: string, symbolB: string, relationSpec: string): Promise<{relation: string, confidence: number}[]> {
    const spec: LLMSpec = {
      symbolId: `link_${symbolA}_${symbolB}`,
      task: `Analyze the relationship between "${symbolA}" and "${symbolB}". 
      
      Consider the relation type: "${relationSpec}"
      
      Provide relationship suggestions in format:
      RELATION: [relationship_name]: [confidence 0-1]: [justification]`,
      context: { linking: true, symbolA, symbolB, relationSpec },
      constraints: { maxTokens: 400, temperature: 0.5 }
    };

    const response = await this.prompt(`link_${symbolA}_${symbolB}`, spec);
    
    // Parse response for relationships
    const relations: {relation: string, confidence: number}[] = [];
    const relationMatches = response.payload.toString().match(/RELATION:\s*([^:]+):\s*([\d.]+):\s*([^\n]+)/gi);
    
    if (relationMatches) {
  relationMatches.forEach((match: string) => {
        const parts = match.match(/RELATION:\s*([^:]+):\s*([\d.]+):\s*([^\n]+)/i);
        if (parts) {
          relations.push({
            relation: parts[1].trim(),
            confidence: parseFloat(parts[2])
          });
        }
      });
    }

    // Default relationship if none found
    if (relations.length === 0) {
      relations.push({
        relation: relationSpec,
        confidence: 0.7
      });
    }

    return relations;
  }

  async embed(payload: any): Promise<number[]> {
    // For now, return a mock embedding
    // In production, you'd call an embedding API
    const text = JSON.stringify(payload);
    const hash = this.simpleHash(text);
    
    // Generate deterministic but varied embedding
    return Array(384).fill(0).map((_, i) => {
      return Math.sin(hash + i) * 0.5;
    });
  }

  private buildSystemPrompt(symbolId: string, spec: LLMSpec): string {
    return `You are an AI agent operating within ΞKernel, a recursive symbolic reasoning system.

CONTEXT:
- Symbol ID: ${symbolId}
- Task: ${spec.task}
- Context: ${JSON.stringify(spec.context)}

INSTRUCTIONS:
1. Your response will become a persistent symbol in a knowledge graph
2. Be specific and actionable - avoid generic responses
3. Consider how your output relates to other symbols
4. Use structured formats when requested (SPAWN:, LINK:, IMPROVE:, etc.)
5. Think step by step and show your reasoning

You are contributing to a living symbolic reasoning process that builds knowledge over time.`;
  }

  private async makeAPICall(request: OpenRouterRequest): Promise<OpenRouterResponse> {
  const response = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
        'HTTP-Referer': 'https://github.com/xi-kernel',
        'X-Title': 'ΞKernel'
      },
      body: JSON.stringify(request)
    });

    if (!response.ok) {
      const error = await response.text();
      throw new Error(`OpenRouter API error (${response.status}): ${error}`);
    }

  return await response.json() as OpenRouterResponse;
  }

  private calculateCost(usage: any, model: string): number {
    // Rough estimates for various models via OpenRouter
    const costs: Record<string, { input: number; output: number }> = {
      'openrouter/sonoma-dusk-alpha': { input: 1 / 1000000, output: 3 / 1000000 },
      'openai/gpt-4o-mini': { input: 0.15 / 1000000, output: 0.6 / 1000000 },
      'anthropic/claude-3-haiku': { input: 0.25 / 1000000, output: 1.25 / 1000000 }
    };

    const modelCost = costs[model] || { input: 1 / 1000000, output: 3 / 1000000 };
    return (usage.prompt_tokens * modelCost.input) + (usage.completion_tokens * modelCost.output);
  }

  private estimateConfidence(finishReason: string | undefined): number {
    switch (finishReason) {
      case 'stop': return 0.9;
      case 'length': return 0.7;
      case 'content_filter': return 0.3;
      default: return 0.6;
    }
  }

  private simpleHash(str: string): number {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // Convert to 32-bit integer
    }
    return Math.abs(hash);
  }
}